{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Synthetic Dataset Generation\n",
    "\n",
    "Here we demonstrate how use our `genalog` package to generate synthetic analog documents with custom image degradation on unlabbled data, which is basically Natural Language rich documents.\n",
    "![genalog_class_diagram](static/unlabeled_synthetic_pipeline.png)\n",
    "\n",
    "### Package Requirements\n",
    "1. Have the `genalog` package installed in your virtual environment:\n",
    "    - Install from source:\n",
    "        1. `git clone https://msazure.visualstudio.com/DefaultCollection/Cognitive%20Services/_git/Tools-Synthetic-Data-Generator`\n",
    "        1. `cd Tools-Synthetic-Data-Generator`\n",
    "        1. `python -m venv .env`\n",
    "        1. `source .env/bin/activate` or on Windows `.env/Scripts/activate.bat`\n",
    "        1. `pip install -r requirements.txt`\n",
    "        1. `pip install -e .`\n",
    "    - Install from Azure Artifacts:\n",
    "        1. Visit this [Azure Artifacts repo](https://msazure.visualstudio.com/DefaultCollection/Cognitive%20Services/_packaging?_a=package&feed=CognitiveServices&package=genalog&protocolType=PyPI&version=0.0.0) and downalod the latest version\n",
    "        1. Relocate the `.whl` package if necessary \n",
    "        1. Create your virtual environment `python -m venv .env` \n",
    "        1. Activate the virtual environemnt `source .env/bin/activate` or on Windows `.env/Script/activate.bat`\n",
    "        1. Run `pip install <GENALOG_WHEEL_NAME>` \n",
    "    \n",
    "1. Download TATK NER model (for generating NER labels for unlabeled data)\n",
    "    1. Please visit see [Cognitive Services Wiki](https://msazure.visualstudio.com/Cognitive%20Services/_wiki/wikis/Cognitive%20Services.wiki/35359/Local-Setup) for details.\n",
    "    1. Specify the path and version of the TA model in the environment variable \"MODEL_ROOT_PATH\" and \"MODEL_VERSION\"\n",
    "1. A collection of **preprocessed text files** from a source dataset. Each text file will be used to generated one synthetic image.\n",
    "    \n",
    "(**Skip** the following steps if you don't want to store documents in the cloud)\n",
    "1. If you haven't already, setup new Azure resources \n",
    "    1. [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) (for storage)\n",
    "    1. [Azure Cognitive Search](https://azure.microsoft.com/en-us/services/search/) (for OCR results)\n",
    "1. Create an `.secret` file with the environment variables that includes the names of you index, indexer, skillset, and datasource to create on the search service. Include keys to the blob that contains the documents you want to index, keys to the congnitive service and keys to you computer vision subscription and search service. In order to index more than 20 documents, you must have a computer services subscription. An example of one such `.secret` file is below:\n",
    "\n",
    "    ```bash\n",
    "    MODEL_ROOT_PATH = <>\n",
    "    MODEL_VERSION = <>\n",
    "    \n",
    "    SEARCH_SERVICE_NAME = \"ocr-ner-pipeline\"\n",
    "    SKILLSET_NAME = \"ocrskillsetcnn\"\n",
    "    INDEX_NAME = \"ocrindexcnn\"\n",
    "    INDEXER_NAME = \"ocrindexercnn\"\n",
    "    DATASOURCE_NAME = \"enkidata\"\n",
    "    DATASOURCE_CONTAINER_NAME = <>\n",
    "    PROJECTIONS_CONTAINER_NAME = <>\n",
    "    COMPUTER_VISION_ENDPOINT = \"https://enki-vision.cognitiveservices.azure.com/\"\n",
    "    COMPUTER_VISION_SUBSCRIPTION_KEY = <>\n",
    "    \n",
    "    BLOB_NAME = \"enkidata\"\n",
    "    BLOB_KEY = <>\n",
    "    SEARCH_QUERY_KEY = <>\n",
    "    SEARCH_SERVICE_KEY = <>\n",
    "    COGNITIVE_SERVICE_KEY = <>\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset file structure \n",
    "\n",
    "Our dataset follows this file structure:\n",
    "```\n",
    "<ROOT FOLDER>/                             #eg. synthetic-image-root\n",
    "     <SRC_DATASET_NAME>                    #eg. CNN-Dailymail-Stories\n",
    "        │\n",
    "        │───shared/                        #common files shared across different dataset versions\n",
    "        │     │───train/\n",
    "        │     │     │───clean_text/  \n",
    "        │     │     │     │─0.txt\n",
    "        │     │     │     │─1.txt\n",
    "        │     │     │     └─...\n",
    "        │     │     └───clean_labels/\n",
    "        │     │           │─0.txt\n",
    "        │     │           │─1.txt\n",
    "        │     │           └─...\n",
    "        │     └───test/\n",
    "        │           │───clean_text/*.txt\n",
    "        │           └───clean_labels/*.txt\n",
    "        │   \n",
    "        └───<VERSION_NAME>/                #e.g. hyphens_blur_heavy\n",
    "               │───train/\n",
    "               │     │─img/*.png           #Degraded Images\n",
    "               │     │─ocr/*.json          #json output files that are output of GROK\n",
    "               │     │─ocr_text/*.txt      #text output retrieved from OCR Json Files\n",
    "               │     └─ocr_labels/*.txt    #Aligned labels files in IOB format\n",
    "               │───test/\n",
    "               │     │─img/*.png           #Degraded Images\n",
    "               │     │─ocr/*.json          #json output files that are output of GROK\n",
    "               │     │─ocr_text/*.txt      #text output retrieved from OCR Json Files\n",
    "               │     └─ocr_labels/*.txt    #Aligned labels files in IOB format\n",
    "               │\n",
    "               │───layout.json             #records page layout info (font-family,template name, etc)\n",
    "               │───degradation.json        #records degradation parameters\n",
    "               │───ocr_metric.csv          #records metrics on OCR noise across the dataset\n",
    "               └───substitution.json       #records character substitution errors in the OCR'ed text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "We will generate the synthetic dataset on your local disk first. You will need to specify the following CONSTANTS to locate where to store the dataset:\n",
    "You will need to put all NL Rich Documents in a folder.\n",
    "If your file is a single large file, you can split it into more sizeable chunks by using the split command. e.g `split -l 5 -a 7 -d big_file.txt`\"\n",
    "You wont be able to Generate labels on large files due to memory issues.\n",
    "\n",
    "1. `INPUT_DATA`: Directory of Unlabbeled Input Data you want to process.\n",
    "1. `ROOT_FOLDER`: root directory of the dataset, path can be relative to the location of this notebook.\n",
    "1. `SRC_DATASET_NAME`: Name the Dataset you are Creating\n",
    "1. **Separate NER labels from document text**: NER labels will be stored in `clean_lables` folder and text in `clean_text` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "INPUT_DATA = \"/data/enki/datasets/CNN_Articles_Cased/demo\"\n",
    "ROOT_FOLDER = \"/data/enki/datasets/synthetic_dataset\"\n",
    "SRC_DATASET_NAME = \"cnn_stories_cased_demo\"\n",
    "CLEAN_TEXT_DIR = os.path.join(ROOT_FOLDER,SRC_DATASET_NAME,\"shared\",\"train\",\"clean_text\")\n",
    "CLEAN_LABEL_DIR = os.path.join(ROOT_FOLDER,SRC_DATASET_NAME,\"shared\",\"train\",\"clean_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "if not os.path.exists(CLEAN_TEXT_DIR):\n",
    "        os.makedirs(CLEAN_TEXT_DIR)\n",
    "if not os.path.exists(CLEAN_LABEL_DIR):\n",
    "        os.makedirs(CLEAN_LABEL_DIR)\n",
    "\n",
    "# Move clean text files to new location\n",
    "print(f\"Copying clean text files \\nfrom {INPUT_DATA} \\nto {CLEAN_TEXT_DIR}\\n\")\n",
    "for filename in tqdm.tqdm(os.listdir(INPUT_DATA)):\n",
    "    base, extension = os.path.splitext(filename)\n",
    "    shutil.copyfile(os.path.join(INPUT_DATA, filename), os.path.join(CLEAN_TEXT_DIR, base+\".txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render example input\n",
    "import os\n",
    "example = sorted(os.listdir(CLEAN_TEXT_DIR))[2]\n",
    "with open(os.path.join(CLEAN_TEXT_DIR, example)) as f:\n",
    "    print(f.read()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels\n",
    "\n",
    "If your dataset has no labels, you will need to generate them first. To do so you will first have to download the tatk NER model and add the path to your model and model version to your environment variable file e.g:\n",
    "\n",
    "```\n",
    "MODEL_ROOT_PATH = \"/mnt/c/Users/dabanda/Downloads/entitygeneral/tatk\"\n",
    "MODEL_VERSION = \"1.0.0.1\"\n",
    "```\n",
    "We have a utility script to download the mode from blob. see `.scripts/download_model.py`\n",
    "\n",
    "After downloading the model, run the label generator. This utility tool will call the model to get the NER labels. The TATK model supports files sizes up to a maximum of 50mb. If your file is above this size, you can split it into more sizeable chunks by using the split command. e.g `split -l 5 -a 7 -d big_file.txt`\n",
    "\n",
    "usage:\n",
    "```\n",
    "python -m genalog.text.label_generator -h\n",
    "\n",
    "usage: label_generator.py [-h] [--use_multiprocesssing USE_MULTIPROCESSSING]\n",
    "                          [--batch_size BATCH_SIZE]\n",
    "                          input_dir output_dir\n",
    "\n",
    "positional arguments:\n",
    "  input_dir             input folder containing text files\n",
    "  output_dir            folder to place label tsv files\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help              show this help message and exit\n",
    "  --use_multiprocesssing  use multiprocessing\n",
    "  --batch_size BATCH_SIZE batch size\n",
    "```\n",
    "\n",
    "Run the generator for train and test sets:\n",
    "\n",
    "```\n",
    "python -m genalog.text.label_generator <ROOT_FOLDER>/<SRC_DATASET_NAME>/shared/train/clean_text/ <ROOT_FOLDER>/<SRC_DATASET_NAME>/shared/train/clean_labels/ --batch_size 5\n",
    "python -m genalog.text.label_generator <ROOT_FOLDER>/<SRC_DATASET_NAME>/shared/train/clean_text/ <ROOT_FOLDER>/<SRC_DATASET_NAME>/shared/train/clean_labels/ --batch_size 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Labels from MT-LSTM Model file\n",
    "We can rely on the TA NER model to provide NER labels for the input text. For the `label_generator` to work, we need the following additional dependencies installed in your virtual environment:\n",
    "```\n",
    "msft-tatk==1.0.122032a1 \n",
    "torch==1.5.0\n",
    "pytorch-pretrained-bert==0.6.2\n",
    "```\n",
    "\n",
    "Some of these are TA's depedencies, you can find instructions to install them from Azure Artifacts [here](https://msazure.visualstudio.com/Cognitive%20Services/_wiki/wikis/Cognitive%20Services.wiki/35359/Local-Setup?anchor=on-windows).\n",
    "\n",
    "PLEASE also remember to setup the following environment variables from above in a `.secrets` file\n",
    "```\n",
    "MODEL_ROOT_PATH = <>\n",
    "MODEL_VERSION = <>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.secrets\")\n",
    "\n",
    "if len(os.listdir(CLEAN_LABEL_DIR)) != 0:\n",
    "    print(f\"CLEAN_LABEL_DIR: {CLEAN_LABEL_DIR} exists. Emptying the existing content\")\n",
    "    existing_files = glob.glob(CLEAN_LABEL_DIR + \"/*.txt\")\n",
    "    for f in existing_files:\n",
    "        os.remove(f)\n",
    "\n",
    "!python -m genalog.text.label_generator $CLEAN_TEXT_DIR $CLEAN_LABEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show after labeling\n",
    "example = sorted(os.listdir(CLEAN_LABEL_DIR))[2]\n",
    "with open(os.path.join(CLEAN_LABEL_DIR, example)) as f:\n",
    "    print(f.read()[:203])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "We will generate the synthetic dataset on your local disk first. You will need to specify the following CONSTANTS to locate where to store the dataset:\n",
    "\n",
    "1. `SRC_TRAIN_SPLIT_PATH`: path of the train-split of the source dataset\n",
    "1. `SRC_TEST_SPLIT_PATH`: path of the test-split of the source dataset\n",
    "1. `VERSION_NAME`: version name of the generated dataset\n",
    "\n",
    "You will also have to define the styles and degradation effects you will like to apply onto each generated document:\n",
    " \n",
    "1. `STYLE_COMBINATIONS`: a dictionary defining the combination of styles to generate per text document (i.e. a copy of the same text document is generate per style combination). Example is shown below:\n",
    "\n",
    "        STYLE_COMBINATION = {\n",
    "        \"language\": [\"en_US\"],\n",
    "        \"font_family\": [\"Segoe UI\"],\n",
    "        \"font_size\": [\"12px\"],\n",
    "        \"text_align\": [\"left\"],\n",
    "        \"hyphenate\": [False],\n",
    "        }\n",
    "    \n",
    "    You can expand the list of each style for more combinations\n",
    "    \n",
    "    \n",
    "2. `DEGRADATIONS`: a list defining the sequence of degradation effects applied onto the synthetic images. Each element is a two-element tuple of which the first element is one of the method names from  `genalog.degradation.effect` and the second element is the corresponding function keyword arguments.\n",
    "\n",
    "        DEGRADATIONS = [\n",
    "            (\"blur\", {\"radius\": 3}),\n",
    "            (\"bleed_through\", {\"alpha\": 0.8}),\n",
    "            (\"morphology\", {\"operation\": \"open\", \"kernel_shape\": (3,3), \"kernel_type\": \"ones\"}), \n",
    "        ]\n",
    "    The example above will apply degradation effects to synthetic images in the sequence of: \n",
    "    \n",
    "            blur -> bleed_through -> morphological operation (open)\n",
    "    \n",
    "   \n",
    "3. `HTML_TEMPLATE`: name of html template used to generate the synthetic images. The `genalog` package has the following default templates: \n",
    "\n",
    "    1. `columns.html.jinja` \n",
    "    2. `letter.html.jinja`\n",
    "    3. `text_block.html.jinja`\n",
    "    \n",
    "            HTML_TEMPLATE = 'text_block.html.jinja'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genalog.degradation.degrader import ImageState\n",
    "\n",
    "VERSION_NAME = \"hyphens_all_heavy\"\n",
    "SRC_TRAIN_SPLIT_PATH = ROOT_FOLDER + \"/\" + SRC_DATASET_NAME + \"/shared/train/clean_text/\"\n",
    "#SRC_TEST_SPLIT_PATH = ROOT_FOLDER +\"/\"+ SRC_DATASET_NAME + \"/shared/test/clean_text/\"\n",
    "DST_TRAIN_PATH = ROOT_FOLDER + \"/\"+ SRC_DATASET_NAME + \"/\" + VERSION_NAME + \"/train/\"\n",
    "#DST_TEST_PATH = ROOT_FOLDER + \"/\"+ SRC_DATASET_NAME + \"/\" + VERSION_NAME + \"/test/\"\n",
    "\n",
    "STYLE_COMBINATIONS = {\n",
    "    \"language\": [\"en_US\"],\n",
    "     \"font_family\": [\"Segeo UI\"],\n",
    "     \"font_size\": [\"12px\"],\n",
    "     \"text_align\": [\"justify\"],\n",
    "     \"hyphenate\": [True],\n",
    "}\n",
    "\n",
    "DEGRADATIONS = [\n",
    "## Elementary Operations\n",
    "#     (\"blur\", {\"radius\": 15}),\n",
    "#     (\"salt\", {\"amount\": 0.7}),\n",
    "#     (\"pepper\", {\"amount\": 0.005}),\n",
    "#     (\"bleed_through\", {\"alpha\": 0.8, \"offset_x\": -5, \"offset_y\": -5,}),\n",
    "#     (\"morphology\", {\"operation\": \"open\", \"kernel_shape\":(9,9), \"kernel_type\":\"plus\"}),\n",
    "    \n",
    "## Stacking Degradations\n",
    "    (\"morphology\", {\"operation\": \"open\", \"kernel_shape\":(9,9), \"kernel_type\":\"plus\"}),\n",
    "    (\"morphology\", {\"operation\": \"close\", \"kernel_shape\":(9,1), \"kernel_type\":\"ones\"}),\n",
    "    (\"salt\", {\"amount\": 0.5}),\n",
    "    (\"overlay\", {\n",
    "        \"src\": ImageState.ORIGINAL_STATE,\n",
    "        \"background\": ImageState.CURRENT_STATE,\n",
    "    }),\n",
    "    (\"bleed_through\", {\n",
    "        \"src\": ImageState.CURRENT_STATE,\n",
    "        \"background\": ImageState.ORIGINAL_STATE,\n",
    "        \"alpha\": 0.8,\n",
    "        \"offset_x\": -12,\n",
    "        \"offset_y\": -8,\n",
    "    }),\n",
    "    (\"pepper\", {\"amount\": 0.015}),\n",
    "    (\"blur\", {\"radius\": 11}),\n",
    "    (\"salt\", {\"amount\": 0.15}),\n",
    "]\n",
    "\n",
    "HTML_TEMPLATE = \"text_block.html.jinja\"\n",
    "\n",
    "IMG_RESOLUTION = 300 #dpi\n",
    "print(f\"Training set will be created from: '{SRC_TRAIN_SPLIT_PATH}'\")\n",
    "print(f\"Training set will be saved to: '{DST_TRAIN_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Text Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "train_text = sorted(glob.glob(SRC_TRAIN_SPLIT_PATH + \"*.txt\"))\n",
    "#test_text = sorted(glob.glob(SRC_TEST_SPLIT_PATH + \"*.txt\"))\n",
    "\n",
    "print(f\"Number of training text documents: {len(train_text)}\")\n",
    "#print(f\"Number of testing text documents: {len(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from genalog.pipeline import AnalogDocumentGeneration\n",
    "from IPython.core.display import Image, display\n",
    "import timeit\n",
    "import cv2\n",
    "\n",
    "sample_file = train_text[0]\n",
    "print(f\"Sample Filename: {sample_file}\")\n",
    "doc_generation = AnalogDocumentGeneration(styles=STYLE_COMBINATIONS, degradations=DEGRADATIONS, resolution=IMG_RESOLUTION)\n",
    "print(f\"Avaliable Templates: {doc_generation.list_templates()}\")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "img_array = doc_generation.generate_img(sample_file, HTML_TEMPLATE, target_folder=None)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(f\"Time to generate 1 documents: {elapsed:.3f} sec\")\n",
    "\n",
    "_, encoded_image = cv2.imencode('.png', img_array)\n",
    "display(Image(data=encoded_image, width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genalog.pipeline import generate_dataset_multiprocess\n",
    "\n",
    "# Generating training set\n",
    "generate_dataset_multiprocess(\n",
    "    train_text, DST_TRAIN_PATH, STYLE_COMBINATIONS, DEGRADATIONS, HTML_TEMPLATE, \n",
    "    resolution=IMG_RESOLUTION, batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Dataset Configurations as .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genalog.pipeline import ImageStateEncoder\n",
    "import json\n",
    "\n",
    "layout_json_path = os.path.join(ROOT_FOLDER,SRC_DATASET_NAME,VERSION_NAME)+\"/layout.json\"\n",
    "degradation_json_path = os.path.join(ROOT_FOLDER,SRC_DATASET_NAME,VERSION_NAME)+\"/degradation.json\"\n",
    "\n",
    "layout = {\n",
    "    \"style_combinations\": STYLE_COMBINATIONS,\n",
    "    \"img_resolution\": IMG_RESOLUTION,\n",
    "    \"html_templates\": [HTML_TEMPLATE],\n",
    "}\n",
    "\n",
    "layout_js_str = json.dumps(layout, indent=2)\n",
    "degrade_js_str = json.dumps(DEGRADATIONS, indent=2, cls=ImageStateEncoder)\n",
    "\n",
    "with open(layout_json_path, \"w\") as f:\n",
    "    f.write(layout_js_str)\n",
    "    \n",
    "with open(degradation_json_path, \"w\") as f:\n",
    "    f.write(degrade_js_str)\n",
    "    \n",
    "print(f\"Writing configs to {layout_json_path}\")\n",
    "print(f\"Writing configs to {degradation_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure Blob Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.secrets\")\n",
    "from genalog.ocr.blob_client import GrokBlobClient\n",
    "\n",
    "local_path = ROOT_FOLDER + \"/\" +SRC_DATASET_NAME\n",
    "remote_path = SRC_DATASET_NAME\n",
    "# dataset_version = VERSION_NAME\n",
    "dataset_version = \"hyphens_all_heavy\"\n",
    "\n",
    "print(f\"Uploadig from local_path: {local_path}\")\n",
    "print(f\"Upload to remote_path:    {remote_path}\")\n",
    "print(f\"dataset_version:          {dataset_version}\")\n",
    "\n",
    "load_dotenv(\"../.secrets\")\n",
    "\n",
    "blob_client = GrokBlobClient.create_from_env_var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset to Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Python uploads can be slow.\n",
    "# for very large datasets use azcopy: https://github.com/Azure/azure-storage-azcopy\n",
    "start = time.time()\n",
    "dest, res = blob_client.upload_images_to_blob(local_path, remote_path, use_async=True)\n",
    "await res\n",
    "print(\"time (mins): \", (time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Indexer and Retrieve OCR results\n",
    "Please note that this process can take a **longer time**, but you can upload multiple dataset to Blob and run this once for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genalog.ocr.rest_client import GrokRestClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.secrets\")\n",
    "grok_rest_client = GrokRestClient.create_from_env_var()\n",
    "grok_rest_client.create_indexing_pipeline()\n",
    "grok_rest_client.run_indexer()\n",
    "\n",
    "# wait for indexer to finish\n",
    "grok_rest_client.poll_indexer_till_complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download OCR Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#blob_img_path_test = os.path.join(remote_path, dataset_version, \"test\", \"img\")\n",
    "blob_img_path_train = os.path.join(remote_path, dataset_version, \"train\", \"img\")\n",
    "#local_ocr_json_path_test = os.path.join(local_path, dataset_version, \"test\", \"ocr\")\n",
    "local_ocr_json_path_train = os.path.join(local_path, dataset_version, \"train\", \"ocr\")\n",
    "#print(f\"Downloading \\nfrom remote path:'{blob_img_path_test} \\n   to local path:'{local_ocr_json_path_test}'\")\n",
    "print(f\"Downloading \\nfrom remote path:'{blob_img_path_train} \\n   to local path:'{local_ocr_json_path_train}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download OCR\n",
    "import os\n",
    "\n",
    "#await blob_client.get_ocr_json(blob_img_path_test, output_folder=local_ocr_json_path_test, use_async=True)\n",
    "await blob_client.get_ocr_json(blob_img_path_train, output_folder=local_ocr_json_path_train, use_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print OCR'ed document\n",
    "import os\n",
    "import json\n",
    "\n",
    "example = sorted(os.listdir(CLEAN_TEXT_DIR))[2]\n",
    "\n",
    "with open(os.path.join(CLEAN_TEXT_DIR, example)) as f:\n",
    "    print(\"****Source text: ****\\n\")\n",
    "    print(f.read()[:500])\n",
    "    \n",
    "with open(os.path.join(local_ocr_json_path_train, example.replace(\"txt\", \"json\"))) as f:\n",
    "    print(\"\\n\\n****OCR'ed text: ****\\n\")\n",
    "    json_data = json.load(f)\n",
    "    print(json_data[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate OCR metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "clean_text_path_template = os.path.join(local_path, \"shared/<test/train>/clean_text\")\n",
    "ocr_json_path_template = os.path.join(local_path, dataset_version, \"<test/train>/ocr\")\n",
    "output_metric_path = os.path.join(local_path, dataset_version)\n",
    "csv_metric_name_template = \"<test/train>_ocr_metrics.csv\"\n",
    "subs_json_name_template = \"<test/train>_subtitutions.json\"\n",
    "avg_metric_name = \"ocr_metrics.csv\"\n",
    "\n",
    "print(f\"Loading \\n'{clean_text_path_template}' \\nand \\n'{ocr_json_path_template}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from genalog.ocr.metrics import get_metrics, substitution_dict_to_json\n",
    "\n",
    "subsets =  [\"train\"]\n",
    "avg_stat = {subset: None for subset in subsets}\n",
    "\n",
    "for subset in subsets:\n",
    "    clean_text_path = clean_text_path_template.replace(\"<test/train>\", subset)\n",
    "    ocr_json_path = ocr_json_path_template.replace(\"<test/train>\", subset)\n",
    "    csv_metric_name = csv_metric_name_template.replace(\"<test/train>\", subset)\n",
    "    subs_json_name = subs_json_name_template.replace(\"<test/train>\", subset)\n",
    "    \n",
    "    output_csv_name = output_metric_path + \"/\" + csv_metric_name\n",
    "    output_json_name = output_metric_path + \"/\" + subs_json_name\n",
    "    \n",
    "    print(f\"Saving to '{output_csv_name}' \\nand '{output_json_name}'\")\n",
    "    \n",
    "    df, subs, actions = get_metrics(clean_text_path, ocr_json_path, use_multiprocessing=True)\n",
    "    df.to_csv(output_csv_name)\n",
    "    json.dump(substitution_dict_to_json(subs), open(output_json_name, \"w\"))\n",
    "    avg_stat[subset] = df.mean()\n",
    "\n",
    "# Saving average metrics\n",
    "avg_stat = pd.DataFrame(avg_stat)\n",
    "output_avg_csv = os.path.join(output_metric_path, avg_metric_name)\n",
    "avg_stat.to_csv(output_avg_csv)\n",
    "print(f\"Saving average metrics to {output_avg_csv}\")\n",
    "print(avg_stat[16:].append(avg_stat[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize OCR'ed Text into IOB Format For Model Training Purpose\n",
    "\n",
    "The last step in preparing the dataset is to format all the OCR'ed text and the NER label into a usable format for training. Our model consume data in IOB format, which is the same format used in the CoNLL datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = local_path\n",
    "degraded_folder = dataset_version\n",
    "print(f\"base_path: {base_path}\\ndegraded_folder: {degraded_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m genalog.text.conll_format $base_path $degraded_folder --train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the final IOB-formatted OCR tokens\n",
    "import os \n",
    "\n",
    "OCR_LABEL_PATH = os.path.join(base_path, degraded_folder, \"train/ocr_labels\")\n",
    "example = sorted(os.listdir(OCR_LABEL_PATH))[2]\n",
    "with open(os.path.join(OCR_LABEL_PATH, example)) as f:\n",
    "    print(\"****Labeled OCR Tokens: ****\\n\")\n",
    "    print(f.read()[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "example = sorted(os.listdir(OCR_LABEL_PATH))[2]\n",
    "with open(os.path.join(CLEAN_LABEL_DIR, example)) as f:\n",
    "    print(\"****Grouth Truth Tokens: ****\\n\")\n",
    "    print(f.read()[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Re-upload Local Dataset to Blob \n",
    "\n",
    "We can re-upload the local copy of the dataset to Blob Storage to sync up the two copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_dataset_to_sync = os.path.join(local_path)\n",
    "blob_path = os.path.join(remote_path)\n",
    "print(f\"local_dataset_to_sync: {local_dataset_to_sync}\\nblob_path: {blob_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Python uploads can be slow.\n",
    "# for very large datasets use azcopy: https://github.com/Azure/azure-storage-azcopy\n",
    "start = time.time()\n",
    "dest, res = blob_client.upload_images_to_blob(local_dataset_to_sync, blob_path, use_async=True)\n",
    "await res\n",
    "print(\"time (mins): \", (time.time()-start)/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tatk",
   "language": "python",
   "name": "tatk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
