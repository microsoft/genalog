{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Generation\n",
    "\n",
    "Here we demonstrate how use our `genalog` package to generate synthetic analog documents with custom image degradation and upload the documents to an Azure Blob Storage.\n",
    "\n",
    "![genalog_class_diagram](static/labeled_synthetic_pipeline.png)\n",
    "\n",
    "### Requirements\n",
    "1. Have the `genalog` package installed in your virtual environment\n",
    "    - Install from source:\n",
    "        1. `git clone https://msazure.visualstudio.com/DefaultCollection/Cognitive%20Services/_git/Tools-Synthetic-Data-Generator`\n",
    "        1. `cd Tools-Synthetic-Data-Generator`\n",
    "        1. `python -m venv .env`\n",
    "        1. `source .env/bin/activate` or on Windows `.env/Scripts/activate.bat`\n",
    "        1. `pip install -r requirements.txt`\n",
    "        1. `pip install -e .`\n",
    "    - Install from Azure Artifacts:\n",
    "        1. Visit this [Azure Artifacts repo](https://msazure.visualstudio.com/DefaultCollection/Cognitive%20Services/_packaging?_a=package&feed=CognitiveServices&package=genalog&protocolType=PyPI&version=0.0.0) and downalod the latest version\n",
    "        1. Relocate the `.whl` package if necessary \n",
    "        1. Create your virtual environment `python -m venv .env` \n",
    "        1. Activate the virtual environemnt `source .env/bin/activate` or on Windows `.env/Script/activate.bat`\n",
    "        1. Run `pip install <GENALOG_WHEEL_NAME>` \n",
    "1. A collection of **preprocessed text files** from a source dataset. Each text file will be used to generated one synthetic image.\n",
    "    \n",
    "(**Skip** the following steps if you don't want to store documents in the cloud)\n",
    "1. If you haven't already, setup new Azure resources \n",
    "    1. [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) (for storage)\n",
    "    1. [Azure Cognitive Search](https://azure.microsoft.com/en-us/services/search/) (for OCR results)\n",
    "1. Create an `.secret` file with the environment variables that includes the names of you index, indexer, skillset, and datasource to create on the search service. Include keys to the blob that contains the documents you want to index, keys to the congnitive service and keys to you computer vision subscription and search service. In order to index more than 20 documents, you must have a computer services subscription. An example of one such `.secret` file is below:\n",
    "\n",
    "    ```bash\n",
    "\n",
    "    SEARCH_SERVICE_NAME = \"ocr-ner-pipeline\"\n",
    "    SKILLSET_NAME = \"ocrskillset\"\n",
    "    INDEX_NAME = \"ocrindex\"\n",
    "    INDEXER_NAME = \"ocrindexer\"\n",
    "    DATASOURCE_NAME = \"syntheticimages\"\n",
    "    DATASOURCE_CONTAINER_NAME = \"ocrimages\"\n",
    "    \n",
    "    COMPUTER_VISION_ENDPOINT = \"https://enki-vision.cognitiveservices.azure.com/\"\n",
    "    COMPUTER_VISION_SUBSCRIPTION_KEY = \"<YOUR SUBSCRIPTION KEY>\"\n",
    "    \n",
    "    BLOB_NAME = \"syntheticimages\"\n",
    "    BLOB_KEY = \"<YOUR BLOB KEY>\"\n",
    "    SEARCH_SERVICE_KEY = \"<YOUR SEARCH SERVICE KEY>\"\n",
    "    COGNITIVE_SERVICE_KEY = \"<YOUR COGNITIVE SERVICE KEY>\"\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset file structure \n",
    "\n",
    "Our dataset follows this file structure:\n",
    "```\n",
    "<ROOT FOLDER>/                             #eg. synthetic-image-root\n",
    "     <SRC_DATASET_NAME>                    #eg. CNN-Dailymail-Stories\n",
    "        │\n",
    "        │───shared/                        #common files shared across different dataset versions\n",
    "        │     │───train/\n",
    "        │     │     │───clean_text/  \n",
    "        │     │     │     │─0.txt\n",
    "        │     │     │     │─1.txt\n",
    "        │     │     │     └─...\n",
    "        │     │     └───clean_labels/\n",
    "        │     │           │─0.txt\n",
    "        │     │           │─1.txt\n",
    "        │     │           └─...\n",
    "        │     └───test/\n",
    "        │           │───clean_text/*.txt\n",
    "        │           └───clean_labels/*.txt\n",
    "        │   \n",
    "        └───<VERSION_NAME>/                #e.g. hyphens_blur_heavy\n",
    "               │───train/\n",
    "               │     │─img/*.png           #Degraded Images\n",
    "               │     │─ocr/*.json          #json output files that are output of GROK\n",
    "               │     │─ocr_text/*.txt      #text output retrieved from OCR Json Files\n",
    "               │     └─ocr_labels/*.txt    #Aligned labels files in IOB format\n",
    "               │───test/\n",
    "               │     │─img/*.png           #Degraded Images\n",
    "               │     │─ocr/*.json          #json output files that are output of GROK\n",
    "               │     │─ocr_text/*.txt      #text output retrieved from OCR Json Files\n",
    "               │     └─ocr_labels/*.txt    #Aligned labels files in IOB format\n",
    "               │\n",
    "               │───layout.json             #records page layout info (font-family,template name, etc)\n",
    "               │───degradation.json        #records degradation parameters\n",
    "               │───ocr_metric.csv          #records metrics on OCR noise across the dataset\n",
    "               └───substitution.json       #records character substitution errors in the OCR'ed text.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Dataset Split\n",
    "\n",
    "Before we can generate analog documents, we need text to populate the analog documents. To do so, we will split the source text into smaller text fragments. Here we have provided a script `genalog.text.splitter` to easily split NER datasets CoNLL-2003 and CoNLL-2012 in the following ways:\n",
    "\n",
    "1. **Split dataset into smaller fragments**: each fragment is named as `<INDEX>.txt`\n",
    "1. **Separate NER labels from document text**: NER labels will be stored in `clean_lables` folder and text in `clean_text` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_TEMPLATE = \"/data/enki/datasets/CoNLL_2003_2012/CoNLL-<DATASET_YEAR>/CoNLL-<DATASET_YEAR>_<SUBSET>.txt\"\n",
    "OUTPUT_FOLDER_TEMPLATE = \"/data/enki/datasets/synthetic_dataset/CoNLL_<DATASET_YEAR>_v3/shared/<SUBSET>/\"\n",
    "for year in [\"2003\", \"2012\"]:\n",
    "    for subset in [\"test\", \"train\"]:\n",
    "        # INPUT_FILE = \"/data/enki/datasets/CoNLL_2003_2012/CoNLL-2012/CoNLL-2012_test.txt\"\n",
    "        INPUT_FILE = INPUT_FILE_TEMPLATE.replace(\"<DATASET_YEAR>\", year).replace(\"<SUBSET>\", subset)\n",
    "        # OUTPUT_FOLDER = \"/data/enki/datasets/synthetic_dataset/CoNLL_2012_v2/shared/test/\"\n",
    "        OUTPUT_FOLDER = OUTPUT_FOLDER_TEMPLATE.replace(\"<DATASET_YEAR>\", year).replace(\"<SUBSET>\", subset)\n",
    "        \n",
    "        print(f\"Loading {INPUT_FILE} \\nOutput to {OUTPUT_FOLDER}\")\n",
    "        if year == \"2003\":\n",
    "            !python -m genalog.text.splitter $INPUT_FILE $OUTPUT_FOLDER --doc_sep=\"-DOCSTART-\\tO\"\n",
    "        else:\n",
    "            !python -m genalog.text.splitter $INPUT_FILE $OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "We will generate the synthetic dataset on your local disk first. You will need to specify the following CONSTANTS to locate where to store the dataset:\n",
    "\n",
    "1. `ROOT_FOLDER`: root directory of the dataset, path can be relative to the location of this notebook.\n",
    "1. `SRC_DATASET_NAME`: name of the source dataset from which the text used in the generation originates from\n",
    "1. `SRC_TRAIN_SPLIT_PATH`: path of the train-split of the source dataset\n",
    "1. `SRC_TEST_SPLIT_PATH`: path of the test-split of the source dataset\n",
    "1. `VERSION_NAME`: version name of the generated dataset\n",
    "\n",
    "You will also have to define the styles and degradation effects you will like to apply onto each generated document:\n",
    " \n",
    "1. `STYLE_COMBINATIONS`: a dictionary defining the combination of styles to generate per text document (i.e. a copy of the same text document is generate per style combination). Example is shown below:\n",
    "\n",
    "        STYLE_COMBINATION = {\n",
    "        \"language\": [\"en_US\"],\n",
    "        \"font_family\": [\"Segoe UI\"],\n",
    "        \"font_size\": [\"12px\"],\n",
    "        \"text_align\": [\"left\"],\n",
    "        \"hyphenate\": [False],\n",
    "        }\n",
    "    \n",
    "    You can expand the list of each style for more combinations\n",
    "    \n",
    "    \n",
    "2. `DEGRADATIONS`: a list defining the sequence of degradation effects applied onto the synthetic images. Each element is a two-element tuple of which the first element is one of the method names from  `genalog.degradation.effect` and the second element is the corresponding function keyword arguments.\n",
    "\n",
    "        DEGRADATIONS = [\n",
    "            (\"blur\", {\"radius\": 3}),\n",
    "            (\"bleed_through\", {\"alpha\": 0.8}),\n",
    "            (\"morphology\", {\"operation\": \"open\", \"kernel_shape\": (3,3), \"kernel_type\": \"ones\"}), \n",
    "        ]\n",
    "    The example above will apply degradation effects to synthetic images in the sequence of: \n",
    "    \n",
    "            blur -> bleed_through -> morphological operation (open)\n",
    "    \n",
    "   \n",
    "3. `HTML_TEMPLATE`: name of html template used to generate the synthetic images. The `genalog` package has the following default templates: \n",
    "\n",
    "    1. `columns.html.jinja` \n",
    "    2. `letter.html.jinja`\n",
    "    3. `text_block.html.jinja`\n",
    "    \n",
    "            HTML_TEMPLATE = 'text_block.html.jinja'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data Over A Range of Degradation Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from genalog.degradation.degrader import ImageState\n",
    "from genalog.pipeline import generate_dataset_multiprocess\n",
    "from genalog.pipeline import AnalogDocumentGeneration\n",
    "from genalog.pipeline import ImageStateEncoder\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "ROOT_FOLDER = \"/data/enki/datasets/synthetic_dataset/\"\n",
    "SRC_DATASET_NAME = \"CoNLL_2003_v3_blur\"\n",
    "HTML_TEMPLATE = \"text_block.html.jinja\"\n",
    "IMG_RESOLUTION = 300 #dpi\n",
    "STYLE_COMBINATIONS = {\n",
    "    \"language\": [\"en_US\"],\n",
    "     \"font_family\": [\"Segeo UI\"],\n",
    "     \"font_size\": [\"12px\"],\n",
    "     \"text_align\": [\"justify\"],\n",
    "     \"hyphenate\": [True],\n",
    "}\n",
    "\n",
    "####################### Specify the range of degradation values to repeat generation over #######################\n",
    "degradation_values = [\n",
    "    \"1\", \"5\", \"9\", \"13\", \"17\", \"21\", \"31\", \"41\", \"51\", \"99\"\n",
    "]\n",
    "################################################################################################################\n",
    "\n",
    "for value in degradation_values:\n",
    "    VERSION_NAME = f\"radius_{value}\"\n",
    "    SRC_TRAIN_SPLIT_PATH = ROOT_FOLDER + SRC_DATASET_NAME + \"/shared/train/clean_text/\"\n",
    "    SRC_TEST_SPLIT_PATH = ROOT_FOLDER + SRC_DATASET_NAME + \"/shared/test/clean_text/\"\n",
    "    DST_TRAIN_PATH = ROOT_FOLDER + SRC_DATASET_NAME + \"/\" + VERSION_NAME + \"/train/\"\n",
    "    DST_TEST_PATH = ROOT_FOLDER + SRC_DATASET_NAME + \"/\" + VERSION_NAME + \"/test/\"\n",
    "    \n",
    "    DEGRADATIONS = [\n",
    "    ## Elementary Operations\n",
    "        (\"blur\", {\"radius\": int(value)}),\n",
    "    #     (\"salt\", {\"amount\": 0.6}),\n",
    "    #     (\"pepper\", {\"amount\": 0.24}),\n",
    "    #     (\"bleed_through\", {\"alpha\": 0.65, \"offset_x\": -6, \"offset_y\": -9}),\n",
    "    #     (\"morphology\", {\"operation\": \"open\", \"kernel_shape\":(6,6), \"kernel_type\":\"plus\"}),\n",
    "    #     (\"morphology\", {\"operation\": \"close\", \"kernel_shape\":(5,1), \"kernel_type\":\"ones\"}),\n",
    "#     (\"morphology\", {\"operation\": \"open\", \"kernel_shape\":(9,9), \"kernel_type\":\"plus\"}),\n",
    "#     (\"morphology\", {\"operation\": \"close\", \"kernel_shape\":(9,1), \"kernel_type\":\"ones\"}),\n",
    "#     (\"salt\", {\"amount\": value.salt_1}),\n",
    "#     (\"overlay\", {\n",
    "#         \"src\": ImageState.ORIGINAL_STATE,\n",
    "#         \"background\": ImageState.CURRENT_STATE,\n",
    "#     }),\n",
    "#     (\"bleed_through\", {\n",
    "#         \"src\": ImageState.CURRENT_STATE,\n",
    "#         \"background\": ImageState.ORIGINAL_STATE,\n",
    "#         \"alpha\": value.bleed_alpha,\n",
    "#         \"offset_x\": -6,\n",
    "#         \"offset_y\": -12,\n",
    "#     }),\n",
    "#     (\"pepper\", {\"amount\": value.pepper}),\n",
    "#     (\"blur\", {\"radius\": value.blur_radius}),\n",
    "#     (\"salt\", {\"amount\": value.salt_2}),\n",
    "    ]\n",
    "    print(f\"Training set will be saved to: '{DST_TRAIN_PATH}'\")\n",
    "    print(f\"Testing set will be saved to: '{DST_TEST_PATH}'\")\n",
    "    \n",
    "    # Load in the dataset\n",
    "    train_text = sorted(glob.glob(SRC_TRAIN_SPLIT_PATH + \"*.txt\"))\n",
    "    test_text = sorted(glob.glob(SRC_TEST_SPLIT_PATH + \"*.txt\"))\n",
    "    print(f\"Number of training text documents: {len(train_text)}\")\n",
    "    print(f\"Number of testing text documents: {len(test_text)}\")\n",
    "    \n",
    "    # Document sample\n",
    "    sample_file = test_text[0]\n",
    "    doc_generation = AnalogDocumentGeneration(styles=STYLE_COMBINATIONS, degradations=DEGRADATIONS, resolution=IMG_RESOLUTION)\n",
    "    img_array = doc_generation.generate_img(sample_file, HTML_TEMPLATE, target_folder=None)\n",
    "    _, encoded_image = cv2.imencode('.png', img_array)\n",
    "    display(Image(data=encoded_image, width=600))\n",
    "    \n",
    "    # Generating test set\n",
    "    generate_dataset_multiprocess(\n",
    "        test_text, DST_TEST_PATH, STYLE_COMBINATIONS, DEGRADATIONS, HTML_TEMPLATE, \n",
    "        resolution=IMG_RESOLUTION, batch_size=5\n",
    "    )\n",
    "    # Generating training set\n",
    "    generate_dataset_multiprocess(\n",
    "        train_text, DST_TRAIN_PATH, STYLE_COMBINATIONS, DEGRADATIONS, HTML_TEMPLATE, \n",
    "        resolution=IMG_RESOLUTION, batch_size=5\n",
    "    )\n",
    "    # Saving configurations\n",
    "    layout_json_path = ROOT_FOLDER + SRC_DATASET_NAME + \"/\" + VERSION_NAME + \"/layout.json\"\n",
    "    degradation_json_path = ROOT_FOLDER + SRC_DATASET_NAME + \"/\" + VERSION_NAME + \"/degradation.json\"\n",
    "    layout = {\n",
    "        \"style_combinations\": STYLE_COMBINATIONS,\n",
    "        \"img_resolution\": IMG_RESOLUTION,\n",
    "        \"html_templates\": [HTML_TEMPLATE],\n",
    "    }\n",
    "    layout_js_str = json.dumps(layout, indent=2)\n",
    "    degrade_js_str = json.dumps(DEGRADATIONS, indent=2, cls=ImageStateEncoder)\n",
    "    with open(layout_json_path, \"w\") as f:\n",
    "        f.write(layout_js_str)\n",
    "    with open(degradation_json_path, \"w\") as f:\n",
    "        f.write(degrade_js_str)\n",
    "    print(f\"Writing configs to {layout_json_path}\")\n",
    "    print(f\"Writing configs to {degradation_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure Blob Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from genalog.ocr.blob_client import GrokBlobClient\n",
    "\n",
    "# Setup variables and authenticate blob client\n",
    "ROOT_FOLDER = \"/data/enki/datasets/synthetic_dataset/\"\n",
    "SRC_DATASET_NAME = \"CoNLL_2003_v3_blur\"\n",
    "\n",
    "local_path = ROOT_FOLDER + SRC_DATASET_NAME \n",
    "remote_path = SRC_DATASET_NAME\n",
    "\n",
    "print(f\"Uploadig from local_path: {local_path}\")\n",
    "print(f\"Upload to remote_path:    {remote_path}\")\n",
    "\n",
    "load_dotenv(\"../.secrets\")\n",
    "\n",
    "blob_client = GrokBlobClient.create_from_env_var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset to Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Python uploads can be slow.\n",
    "# for very large datasets use azcopy: https://github.com/Azure/azure-storage-azcopy\n",
    "start = time.time()\n",
    "dest, res = blob_client.upload_images_to_blob(local_path, remote_path, use_async=True)\n",
    "await res\n",
    "print(\"time (mins): \", (time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a remote folder on Blob\n",
    "# blob_client.delete_blobs_folder(\"cnndm_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Indexer and Retrieve OCR results\n",
    "Please note that this process can take a **long time**, but you can upload multiple dataset to Blob and run this once for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from genalog.ocr.rest_client import GrokRestClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.secrets\")\n",
    "grok_rest_client = GrokRestClient.create_from_env_var()\n",
    "grok_rest_client.create_indexing_pipeline()\n",
    "grok_rest_client.run_indexer()\n",
    "\n",
    "# wait for indexer to finish\n",
    "grok_rest_client.poll_indexer_till_complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download OCR Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Downloading multiple dataset to local\n",
    "remote_path = SRC_DATASET_NAME\n",
    "local_path = ROOT_FOLDER + SRC_DATASET_NAME\n",
    "versions = [\"1\", \"5\", \"9\", \"13\", \"17\", \"21\", \"31\", \"41\", \"51\", \"99\"]\n",
    "version_prefix = \"radius_\"\n",
    "version_suffixes = [\"\"]\n",
    "print(f\"Remote Path: {remote_path} \\nLocal Path: {local_path} \\nVersions: {versions}\")\n",
    "\n",
    "blob_img_paths_test = []\n",
    "blob_img_paths_train = []\n",
    "local_ocr_json_paths_test = []\n",
    "local_ocr_json_paths_train = []\n",
    "version_name = \"\"\n",
    "for version in versions:\n",
    "    for weight in version_suffixes:\n",
    "        version_name = version_prefix + version + weight\n",
    "        blob_img_paths_test.append(os.path.join(remote_path, version_name, \"test\", \"img\"))\n",
    "        blob_img_paths_train.append(os.path.join(remote_path, version_name, \"train\", \"img\"))\n",
    "        local_ocr_json_paths_test.append(os.path.join(local_path, version_name, \"test\", \"ocr\"))\n",
    "        local_ocr_json_paths_train.append(os.path.join(local_path, version_name, \"train\", \"ocr\"))\n",
    "print(f\"Example Version Name: {version_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download OCR\n",
    "for blob_path_test, blob_path_train, local_path_test, local_path_train in \\\n",
    "    zip(blob_img_paths_test, blob_img_paths_train, \\\n",
    "        local_ocr_json_paths_test, local_ocr_json_paths_train):\n",
    "        \n",
    "    print(f\"Downloading \\nfrom remote path:'{blob_path_test} \\n   to local path:'{local_path_test}'\")\n",
    "    await blob_client.get_ocr_json(blob_path_test, output_folder=local_path_test, use_async=True)\n",
    "    print(f\"Downloading \\nfrom remote path:'{blob_path_train} \\n   to local path:'{local_path_train}'\")\n",
    "    await blob_client.get_ocr_json(blob_path_train, output_folder=local_path_train, use_async=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate OCR metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "local_path = ROOT_FOLDER + SRC_DATASET_NAME\n",
    "versions = [\"1\", \"5\", \"9\", \"13\", \"17\", \"21\", \"31\", \"41\", \"51\", \"99\"]\n",
    "version_prefix = \"radius_\"\n",
    "version_suffixes = [\"\"]\n",
    "print(f\"Local Path: {local_path} \\nVersions: {versions}\\n\")\n",
    "\n",
    "# local_path = \"/data/enki/datasets/synthetic_dataset/CoNLL_2003_v3/\"\n",
    "# dataset_version = \"hyphens_no_degradation\"\n",
    "input_json_path_templates = []\n",
    "output_metric_path = []\n",
    "for version in versions:\n",
    "    for suffix in version_suffixes:\n",
    "        version_name = version_prefix + version + suffix\n",
    "        # Location depends on the input dataset\n",
    "        input_json_path_templates.append(os.path.join(local_path, version_name, \"<test/train>/ocr\"))\n",
    "        output_metric_path.append(os.path.join(local_path, version_name))\n",
    "        \n",
    "clean_text_path_template = os.path.join(local_path, \"shared/<test/train>/clean_text\")\n",
    "csv_metric_name_template = \"<test/train>_ocr_metrics.csv\"\n",
    "subs_json_name_template = \"<test/train>_subtitutions.json\"\n",
    "avg_metric_name = \"ocr_metrics.csv\"\n",
    "\n",
    "print(f\"Loading \\n'{clean_text_path_template}' \\nand \\n'{input_json_path_templates[0]}'...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from genalog.ocr.metrics import get_metrics, substitution_dict_to_json\n",
    "\n",
    "for input_json_path_template, output_metric_path in zip(input_json_path_templates, output_metric_path):\n",
    "    subsets = [\"test\", \"train\"]\n",
    "    avg_stat = {subset: None for subset in subsets}\n",
    "    for subset in subsets:\n",
    "        clean_text_path = clean_text_path_template.replace(\"<test/train>\", subset)\n",
    "        ocr_json_path = input_json_path_template.replace(\"<test/train>\", subset)\n",
    "        csv_metric_name = csv_metric_name_template.replace(\"<test/train>\", subset)\n",
    "        subs_json_name = subs_json_name_template.replace(\"<test/train>\", subset)\n",
    "\n",
    "        output_csv_name = output_metric_path + \"/\" + csv_metric_name\n",
    "        output_json_name = output_metric_path + \"/\" + subs_json_name\n",
    "\n",
    "        print(f\"Saving to '{output_csv_name}' \\nand '{output_json_name}'\")\n",
    "        df, subs, actions = get_metrics(clean_text_path, ocr_json_path, use_multiprocessing=True)\n",
    "        # Writing metrics on individual file\n",
    "        df.to_csv(output_csv_name)\n",
    "        json.dump(substitution_dict_to_json(subs), open(output_json_name, \"w\"))\n",
    "        # Getting average metrics\n",
    "        avg_stat[subset] = df.mean()\n",
    "\n",
    "    # Saving average metrics\n",
    "    avg_stat = pd.DataFrame(avg_stat)\n",
    "    output_avg_csv = os.path.join(output_metric_path, avg_metric_name)\n",
    "    avg_stat.to_csv(output_avg_csv)\n",
    "    print(f\"Saving average metrics to {output_avg_csv}\")\n",
    "    print(avg_stat[16:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize OCR'ed Text into IOB Format For Model Training Purpose\n",
    "\n",
    "The last step in preparing the dataset is to format all the OCR'ed text and the NER label into a usable format for training. Our model consume data in IOB format, which is the same format used in the CoNLL datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = ROOT_FOLDER + SRC_DATASET_NAME\n",
    "local_path = \"/data/enki/datasets/synthetic_dataset/CoNLL_2012_v3\"\n",
    "versions = [\"1\", \"5\", \"9\", \"13\", \"17\", \"21\", \"31\", \"41\", \"51\", \"99\"]\n",
    "version_prefix = \"radius_\"\n",
    "version_suffixes = [\"\"]\n",
    "version_names = []\n",
    "for version in versions:\n",
    "    for suffix in version_suffixes:\n",
    "        version_names.append(version_prefix + version + suffix)\n",
    "print(f\"base_path: {base_path}\\nversion_names: {version_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for version_name in version_names:\n",
    "    print(f\"Formatting {version_name}\")\n",
    "    !python -m genalog.text.conll_format $local_path $version_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Re-upload Local Dataset to Blob \n",
    "\n",
    "We can re-upload the local copy of the dataset to Blob Storage to sync up the two copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_dataset_to_sync = os.path.join(local_path)\n",
    "blob_path = os.path.join(remote_path)\n",
    "print(f\"local_dataset_to_sync: {local_dataset_to_sync}\\nblob_path: {blob_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Python uploads can be slow.\n",
    "# for very large datasets use azcopy: https://github.com/Azure/azure-storage-azcopy\n",
    "start = time.time()\n",
    "dest, res = blob_client.upload_images_to_blob(local_dataset_to_sync, blob_path, use_async=True)\n",
    "await res\n",
    "print(\"time (mins): \", (time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a remote folder on Blob\n",
    "# blob_client.delete_blobs_folder(blob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
